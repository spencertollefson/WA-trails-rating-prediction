{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "\n",
    "firefoxdriver = \"/usr/local/bin/geckodriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.firefox.driver\"] = firefoxdriver\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'png' # ‘png’, ‘retina’, ‘jpeg’, ‘svg’, ‘pdf’\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.wta.org/go-hiking/hikes/wilderness-peak'\n",
    "pacurl = 'https://www.wta.org/go-hiking/hikes/pacific-northwest-trail'\n",
    "response = requests.get(url)\n",
    "response2 = requests.get(pacurl)\n",
    "\n",
    "page = response.text\n",
    "page2 = response2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac = BeautifulSoup(page2, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\n",
    "    'name',\n",
    "    'region',\n",
    "    'subregion',\n",
    "    'votes',\n",
    "    'rating',\n",
    "    'length',\n",
    "    'lengthtype',\n",
    "    'gain',\n",
    "    'hpoint',\n",
    "    'fee',\n",
    "    'lat',\n",
    "    'long',\n",
    "    'trailhead1',\n",
    "    'trailhead2',\n",
    "    'author1',\n",
    "    'author2',\n",
    "    'countreports',\n",
    "          ]\n",
    "\n",
    "binary_headers = [\n",
    "    'Wildflowers/Meadows',\n",
    "    'Ridges/passes',\n",
    "    'Wildlife',\n",
    "    'Waterfalls',\n",
    "    'Old growth',\n",
    "    'Summits',\n",
    "    'Good for kids',\n",
    "    'Dogs allowed on leash',\n",
    "    'Fall foliage',\n",
    "    'Lakes',\n",
    "    'Rivers',\n",
    "    'Coast',\n",
    "    'Mountain views',\n",
    "    'Established campsites',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_hike_stat(soup, stat):\n",
    "    obj = soup.find(text=re.compile(stat))\n",
    "    if not obj:\n",
    "        return None\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(soup):\n",
    "    return soup.find(class_='documentFirstHeading').text\n",
    "\n",
    "def get_region(soup):\n",
    "    regions = get_soup_hike_stat(soup, 'Location').find_next().text\n",
    "    regions = regions.split(sep=\"--\")\n",
    "    return regions[0].strip()\n",
    "\n",
    "def get_subregion(soup):\n",
    "    regions = get_soup_hike_stat(soup, 'Location').find_next().text\n",
    "    regions = regions.split(sep=\"--\")\n",
    "    return regions[1].strip()\n",
    "\n",
    "def get_length(soup):\n",
    "    length = get_soup_hike_stat(soup, 'Length').find_next().find('span').text\n",
    "    return length.split()[0]\n",
    "\n",
    "def get_lengthtype(soup):\n",
    "    length = get_soup_hike_stat(soup, 'Length').find_next().find('span').text\n",
    "    return length.split()[2]\n",
    "\n",
    "def get_gain(soup):\n",
    "    return get_soup_hike_stat(soup, 'Gain').find_next().text\n",
    "\n",
    "def get_hpoint(soup):\n",
    "    return get_soup_hike_stat(soup, 'Highest Point').find_next().text\n",
    "\n",
    "def get_fee(soup):\n",
    "    return get_soup_hike_stat(soup, 'Parking Pass/Entry Fee').find_next().text\n",
    "\n",
    "def get_lat(soup):\n",
    "    return get_soup_hike_stat(soup, 'Co-ordinates').find_next().text\n",
    "\n",
    "def get_long(soup):    \n",
    "    return get_soup_hike_stat(soup, 'Co-ordinates').find_next().find_next().text\n",
    "\n",
    "def get_trailhead1(soup):\n",
    "    obj = soup.find(id='trailhead-details').find_all('p')[1].text\n",
    "    if \"weather\" in obj:\n",
    "        return np.nan\n",
    "    return obj\n",
    "def get_trailhead2(soup):\n",
    "    obj = soup.find(id='trailhead-details').find_all('p')[2].text\n",
    "    if \"weather\" in obj:\n",
    "        return np.nan\n",
    "    return obj\n",
    "\n",
    "def get_author1(soup):\n",
    "    return soup.find(class_='authorship sidebar-section').find('p').find_all('span')[0].text\n",
    "\n",
    "def get_author2(soup):\n",
    "    return soup.find(class_='authorship sidebar-section').find('p').find_all('span')[1].text\n",
    "\n",
    "def get_countreports(soup):\n",
    "    return soup.find(class_='ReportCount').text\n",
    "\n",
    "def get_votes(soup):\n",
    "    return soup.find(class_='rating-count').text.split()[0][1:]\n",
    "\n",
    "def get_rating(soup):\n",
    "    return soup.find(class_='current-rating').text.split()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to mark if certain categorical features are present\n",
    "\n",
    "## Only run this after having created the dictionary for the current webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_features(soup, headers):\n",
    "    '''\n",
    "    soup: beautifulsoup object of web page\n",
    "    headers: list of names of the particular wta.org trail page features we want.\n",
    "    ----\n",
    "    output is defaultdict\n",
    "    '''\n",
    "    feat_dict = defaultdict()\n",
    "    for i in headers:\n",
    "        try:\n",
    "            # allows to call methods by inserting a string into the func name\n",
    "            feat_dict[i] = [globals()[\"get_\" + i](soup)]\n",
    "        except:\n",
    "            feat_dict[i] = np.nan\n",
    "    return feat_dict\n",
    "\n",
    "def append_binary_feats(soup, binary_headers, feat_dict):\n",
    "    '''\n",
    "    soup: beautifulsoup object of web page\n",
    "    datadict: this is the dictionary used for each webpage to store the variables.\n",
    "    Thus this function should be called only after that dict has been created.\n",
    "    ----\n",
    "    Product: this outcome appends binary categorical features present as a \"1\" or \"0\" to an existing\n",
    "    dictionary\n",
    "    '''\n",
    "    bin_features = binary_headers.copy()\n",
    "    for i in soup.find_all(class_='feature'):\n",
    "        label = i.attrs['data-title']\n",
    "        if label in bin_features:\n",
    "            feat_dict[label] = 1\n",
    "            bin_features.remove(label)\n",
    "    for remainder in bin_features:\n",
    "        feat_dict[remainder] = 0\n",
    "    return feat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine into one scrape function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(soup, headers=headers, binary_headers=binary_headers):\n",
    "    feat_dict = append_features(soup, headers)\n",
    "    feat_dict = append_binary_feats(soup, binary_headers, feat_dict)\n",
    "    # Create pandas DataFrame from dictionary\n",
    "    df_page = pd.DataFrame.from_dict(feat_dict)\n",
    "    return df_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_to_master(df_page, old_df=None):\n",
    "    if old_df is not None:\n",
    "        return pd.concat([old_df, df_page], join='inner')\n",
    "    else:\n",
    "        print(\"only 1 DataFrame present - returning it back to you\")\n",
    "        return df_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup and Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_page_df = None\n",
    "masterdf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through url from 0 to 3540, in increments of 30\n",
    "# End of URL has simple integer that counts upward by 30 on each \"Next 30\" page of list of trails\n",
    "# https://www.wta.org/go-outside/hikes/?b_start:int=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must substantiate first row of df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is it! The following cell creates the WTA df with all the data.\n",
    "\n",
    "### **WARNING!!** Cell will take ~ 1 hour and 9 minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_create_all_wta():\n",
    "    '''\n",
    "    Function scrapes every WTA hike page, concatenates info, and returns Pandas DF.\n",
    "    WARNING!!! This function takes approximately 1 hours and 9 minutes to run\n",
    "    '''\n",
    "    # tripreports url changes by number at end. iterates by 30 - begins at 0, ends at 3540\n",
    "    df = None\n",
    "    for i in range(0, 3541, 30): \n",
    "        wtaurl = f'https://www.wta.org/go-outside/hikes/?b_start:int={i}'\n",
    "        wtaresponse = requests.get(wtaurl)\n",
    "        wta_html = wtaresponse.text\n",
    "        wta_soup = BeautifulSoup(wta_html, 'lxml')\n",
    "        for j in wta_soup.find_all(class_='listitem-title'):\n",
    "            page_url = j['href']\n",
    "            page_response = requests.get(page_url)\n",
    "            page_html = page_response.text\n",
    "            page_soup = BeautifulSoup(page_html, 'lxml')\n",
    "            new_page_df = scrape_page(page_soup)\n",
    "            df = concat_to_master(new_page_df, df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only 1 DataFrame present - returning it back to you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3555, 31)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = scrape_and_create_all_wta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle to save to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/raw_wta_df.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(dfcopy, picklefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "22px",
    "width": "211.333px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
